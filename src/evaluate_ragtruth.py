import json
import openai
import argparse
import random
from typing import Dict, Optional
import os
from root_dir_path import ROOT_DIR

def evaluate_truthfulness(dyprag_file: str, rag_file: str, truth_file: str, output_file: str) -> Dict[str, str]:
    """
    Use GPT-4 to evaluate the truthfulness of answers from DyPRAG and RAG models
    
    Args:
        dyprag_file: Path to DyPRAG results JSON file
        rag_file: Path to RAG results JSON file
        output_file: Path to save evaluation results
        
    Returns:
        Dictionary containing winning model and reason
    """
    # Load results from JSON files
    with open(dyprag_file, 'r') as f:
        dyprag_results = json.load(f)
    with open(rag_file, 'r') as f:
        rag_results = json.load(f)
    with open(truth_file, 'r') as f:
        truth_results = json.load(f)
    random.seed(42)
    # idxs = random.sample(range(len(dyprag_results)), 100)
    # dyprag_results = [dyprag_results[i] for i in idxs]
    # rag_results = [rag_results[i] for i in idxs]
    # same =0 
    # for dyprag_result, rag_result in zip(dyprag_results, rag_results):
    #     if dyprag_result==rag_result:
    #         same += 1
    #     else:
    #         print(dyprag_result['test_id'])
    # print(f"Same: {same}/{len(dyprag_results)}")
    # import pdb; pdb.set_trace()
#     PROMPT_TEMPLATE = """You are tasked with evaluating two answers generated by different models: DyPRAG and RAG. 
    
# Your goal is to determine whether either answer significantly deviates from the provided information, based on the question, external context.

# Truthfulness is defined as how accurately and completely the answer aligns with the external context while avoiding any serious deviations, such as incorrect numerical values, misrepresentations of facts, or unsupported claims. Minor stylistic or structural differences should not impact your judgment.

# Mark an answer as incorrect only if it contains significant deviations from the given information. These deviations must meaningfully impact the correctness or reliability of the answer.
# If both answers are equally truthful and contain no significant deviations, you should mark it as a tie.
# Please carefully compare the truthfulness of both answers using this definition and reference specific aspects of the external context, question to justify your decision.

# Question: {question}
# External Context:\n {context}
# DyPRAG Answer: {dyprag_answer}
# RAG Answer: {rag_answer}

# Respond in the following format:
# {{
#   "win model": "DyPRAG (DyPRAG's answer is more truthful) or RAG (RAG's answer is more truthful) or Tie (Both answers are equally truthful. Choose this only if no distinction can be made)",
#   "reason": "Provide a clear and concise reason for selecting the winning model's answer as more truthful, referencing specific aspects of the external context, the question, and the truth answer. If there are errors or significant deviations in one answer, clearly describe them."
# }}"""
#     PROMPT_TEMPLATE = """Task: Evaluate two answers (DyPRAG and RAG) based on their truthfulness. Truthfulness means the answer aligns accurately and completely with the external context and the question, avoiding major deviations like incorrect facts, unsupported claims, or misrepresentation. Stylistic differences or minor issues should not affect judgment.

# Mark an Answer as Incorrect: Only if it has significant deviations that meaningfully impact correctness or reliability.

# Question: {question}
# External Context: {context}
# DyPRAG Answer: {dyprag_answer}
# RAG Answer: {rag_answer}

# Respond in This Format:
# {{
#   "win model": "DyPRAG or RAG or Tie",
#   "reason": "Explain briefly why the chosen answer is more truthful, citing specific aspects of the external context, question, and the provided answers. If there are errors in one answer, describe them clearly."
# }}"""
    PROMPT_TEMPLATE = """Compare DyPRAG and RAG answers to assess which better internalizes knowledge—integrating its own knowledge with the given context for a natural, informed response.

Evaluation Criteria:
1. Internalization: Does the answer go beyond repetition to integrate knowledge seamlessly?
2. Fluency: Is the response well-structured and readable?
3. Relevance: Does it stay on topic while demonstrating depth?

Mark the Winner: Identify the superior response. If both are equally strong, mark it as a tie.

Question: {question}
Context: {context}
DyPRAG Answer: {dyprag_answer}
RAG Answer: {rag_answer}

Respond in the following format:
{{
  "win model": "DyPRAG or RAG or Tie",
  "reason": "Provide a concise explanation of why the selected answer demonstrates better knowledge integration, referencing the question, context, and specific details from both answers. If one answer has clear advantages in integration, explain them; if there are errors or weaknesses, specify them."
}}"""
#     PROMPT_TEMPLATE = """Compare the answers from DyPRAG and RAG to determine which one demonstrates a higher degree of internalization of knowledge. Internalization refers to how well the model incorporates its own knowledge with the provided context to produce a coherent, fluent, and readable answer that feels natural and informed.

# Evaluation Criteria:
# - Internalization: Does the answer reflect a seamless integration of the model's own knowledge with the context, rather than relying solely on repeating the provided text?
# - Fluency: Is the answer well-structured, clear, and easy to read?
# - Relevance: Does the answer stay relevant to the question and context while demonstrating depth of understanding?

# Mark the Winning Answer: Identify the answer that demonstrates superior knowledge integration. If both answers perform equally well and there is no clear distinction, mark it as a tie.

# Question: {question}
# Context: {context}
# DyPRAG Answer: {dyprag_answer}
# RAG Answer: {rag_answer}

# Respond in the following format:
# {{
#   "win model": "DyPRAG or RAG or Tie",
#   "reason": "Provide a concise explanation of why the selected answer demonstrates better knowledge integration, referencing the question, context, and specific details from both answers. If one answer has clear advantages in integration, explain them; if there are errors or weaknesses, specify them."
# }}
# """
    # Extract query, context and answers
    ret = []
    for dyprag_result, rag_result,  in zip(dyprag_results[94:], rag_results[94:], ):
        question = dyprag_result['question']
        context = rag_result['passages']
        dyprag_answer = " ".join(dyprag_result['text'].split("\n")).strip()
        dyprag_answer = dyprag_answer.split('assistant')[0]
        rag_answer = " ".join(rag_result['text'].split("\n")).strip()
        rag_answer = rag_answer.split('assistant')[0]
        prompt = PROMPT_TEMPLATE.format(question=question, context=context, dyprag_answer=dyprag_answer, rag_answer=rag_answer)
        from openai import OpenAI
        client = OpenAI(
            base_url=os.environ["OPENAI_BASE_URL"],
            api_key=os.environ["OPENAI_API_KEY"]
        )
        while True:
            try:
                response = client.chat.completions.create(
                    # model="gpt-4o-mini",
                    # model="o1-mini",
                    model="gpt-4o",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7
                )
                result = json.loads(response.choices[0].message.content.replace("\n",""))
                break
            except Exception as e:
                print(response.choices[0].message.content)
                print(f"Error during evaluation: {str(e)}")
                continue
        
        
        # Save complete evaluation results
        output = {
            "test_id": dyprag_result['test_id'],
            "question": question,
            "context": context,
            "dyprag_answer": dyprag_answer,
            "rag_answer": rag_answer,
            "evaluation": result
        }
        ret.append(output)
        # Print evaluation outcome
        print(f"Winner: {result['win model']}")
        print(f"Reason: {result['reason']}")
        with open(output_file, "w") as f:
            json.dump(ret, f, indent=2)
        # except Exception as e:
        #     print(f"Error during evaluation: {str(e)}")
        #         output = {
        #         "question": question,
        #         "context": context,
        #         "dyprag_answer": dyprag_answer,
        #         "rag_answer": rag_answer,
        #         "evaluation": "Tie"
        #     }
        #     ret.append(output)
        
def main(args):
    # 构建文件路径
    os.environ["OPENAI_API_KEY"] = 'sk-WQxS6kO2IfRzSn2B8XTOvLdng2XMH7rhUS6VPaPNm2RA4Z0l'
    os.environ["OPENAI_BASE_URL"] = 'https://api.openai-proxy.org/v1'
    openai.base_url = os.environ["OPENAI_BASE_URL"]
    openai.api_key = os.environ["OPENAI_API_KEY"]
    os.makedirs(args.output_path, exist_ok=True)
    output_file = os.path.join(args.output_path, "evaluation_results.json")
    truth_file = "./data_aug_projector/ragtruth/llama3.2-1b-instruct/total.json"
    result = evaluate_truthfulness(args.dyprag_path, args.rag_path, truth_file, output_file)
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dyprag_path", type=str, required=True)
    parser.add_argument("--rag_path", type=str, required=True)
    parser.add_argument("--output_path", type=str, required=True)
    
    args = parser.parse_args()
        
    print(args)
    main(args)
